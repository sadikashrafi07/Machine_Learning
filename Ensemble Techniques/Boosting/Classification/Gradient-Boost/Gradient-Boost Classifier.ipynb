{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c07380",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86896968",
   "metadata": {},
   "source": [
    "Gradient boosting is an alternative form of Adaboost. Many consider gradient boosting to be a better performer than adaboost.Some differences between the two algorithms is that gradient boosting uses optimization for weight the estimators. Like adaboost, gradient boosting can be used for most algorithms but is commonly associated with decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7829a24",
   "metadata": {},
   "source": [
    "In addition , gradient boosting requires several additional hyper parameters such as max depth and subsample. Max depth has to do with the number of nodes in a tree. The higher the number the purer the classification become. The downside to this is the risk of overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b267813a",
   "metadata": {},
   "source": [
    "Subsampling has to do with the proportion of the sample that is used for each estimator. This can range form a decimal value up until the whole number 1. If the value is set to 1 it becomes stochastic gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13c993",
   "metadata": {},
   "source": [
    "This post is focused on classification. To do this, we will use the cancer dataset from the pydataset library. Our goal will be to predict the status of patients(alive or dead)using the available independent variables. The steps we will use as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4f02ce",
   "metadata": {},
   "source": [
    "- Data Preparation\n",
    "- Baseline decision tree model\n",
    "- Hyperparameter tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc27ee47",
   "metadata": {},
   "source": [
    "#### Importing neccesary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c2e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c445f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from pydataset import data\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc206dfb",
   "metadata": {},
   "source": [
    "The data preperation is simple in this situation. All we need to do is load are dataset, dropping missing values, and create our X dataset and y dataset. All this happens in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fab24e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data('cancer').dropna()\n",
    "X = df[['time','sex','ph.karno','pat.karno', 'meal.cal', 'wt.loss']]\n",
    "y = df['status']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994bafd7",
   "metadata": {},
   "source": [
    "We will create now our baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501489f",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d26252",
   "metadata": {},
   "source": [
    "The purpose of the baseline model is to have something to compare our gradient boosting model to. The strength of a model is always relative to some other model, so we need to make at least two, so we can say one is better than other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd4827e",
   "metadata": {},
   "source": [
    "The criteria in this situation is accuracy.Therefore, we will make a decision tree model. But we will manipulate the max depth of the tree to create 9 different baseline models. The best accuracy model will be the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a51af6",
   "metadata": {},
   "source": [
    "To achieve this we need to use a for loop to make python make several decision trees. We also need to set the parameters for the cross validation by calling KFold(). Once this done, we print the result for the 9 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808e38fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.71875\n",
      "2 0.6477941176470589\n",
      "3 0.6768382352941177\n",
      "4 0.6698529411764707\n",
      "5 0.6584558823529412\n",
      "6 0.6525735294117647\n",
      "7 0.6283088235294118\n",
      "8 0.6573529411764706\n",
      "9 0.6577205882352941\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=10,shuffle=True,random_state=1)\n",
    "for depth in range(1,10):\n",
    "    tree_classifier = tree.DecisionTreeClassifier(max_depth = depth, random_state = 1)\n",
    "    if tree_classifier.fit(X,y).tree_.max_depth<depth:\n",
    "        break\n",
    "    score = np.mean(cross_val_score(tree_classifier,X,y,scoring = 'accuracy',cv = crossvalidation,n_jobs = 1))\n",
    "    print(depth,score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b400c55",
   "metadata": {},
   "source": [
    "It appears that when the max depth is limited to 1 that we get the best accuracy at almost  72%. This will be our baseline for comparison. We will tune the parameters for the gradient boosting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e060aee",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86369bd8",
   "metadata": {},
   "source": [
    "There are several hyperparameters we need to tune. The ones we will tune as fallows\n",
    "- number of estimators \n",
    "- learning rate \n",
    "- subsample\n",
    "- max_depth\n",
    "\n",
    "First we will create an instance of the gradient boosting classifier.Second we will create our grid for the search. It is inside this grid that we set several values for each hyperparameter. Then we call GridSearchCV and place the instance of the gradient boosting classifier, the grid, the cross validation values from mad earlier, and n_jobs all together in one place. Below is the code for this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81765ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = {\n",
    "    'n_estimators':[500,1000,2000],\n",
    "    'learning_rate':[.001,0.01,.1],\n",
    "    'max_depth':[1,3,5],\n",
    "    'subsample':[.5,.75,1],\n",
    "    'random_state':[1]\n",
    "}\n",
    "\n",
    "classifier = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6152aec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=KFold(n_splits=10, random_state=1, shuffle=True),\n",
       "             estimator=GradientBoostingClassifier(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                         &#x27;max_depth&#x27;: [1, 3, 5],\n",
       "                         &#x27;n_estimators&#x27;: [500, 1000, 2000], &#x27;random_state&#x27;: [1],\n",
       "                         &#x27;subsample&#x27;: [0.5, 0.75, 1]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=KFold(n_splits=10, random_state=1, shuffle=True),\n",
       "             estimator=GradientBoostingClassifier(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                         &#x27;max_depth&#x27;: [1, 3, 5],\n",
       "                         &#x27;n_estimators&#x27;: [500, 1000, 2000], &#x27;random_state&#x27;: [1],\n",
       "                         &#x27;subsample&#x27;: [0.5, 0.75, 1]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=10, random_state=1, shuffle=True),\n",
       "             estimator=GradientBoostingClassifier(),\n",
       "             param_grid={'learning_rate': [0.001, 0.01, 0.1],\n",
       "                         'max_depth': [1, 3, 5],\n",
       "                         'n_estimators': [500, 1000, 2000], 'random_state': [1],\n",
       "                         'subsample': [0.5, 0.75, 1]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiercv=GridSearchCV(classifier,param_grid=parameter,cv=crossvalidation,scoring='accuracy')\n",
    "classifiercv.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3cfc06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.001,\n",
       " 'max_depth': 5,\n",
       " 'n_estimators': 1000,\n",
       " 'random_state': 1,\n",
       " 'subsample': 0.75}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiercv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b57224e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7301470588235295"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiercv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b773f2f8",
   "metadata": {},
   "source": [
    "You can see that the best hyper parameters are for yourself. In addition, we see that when these parameters were set we got an accuracy of 74%. This is superior to our baseline model.We will now see if we can replicate these numbers when we use them for our Gradient Boosting model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e690e",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d930985a",
   "metadata": {},
   "source": [
    "Below is the code and results for the model with the predetermined hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b910fbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7301470588235295"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada2 = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.001, subsample=0.75, max_depth=5, random_state=1)\n",
    "score = np.mean(cross_val_score(ada2,X,y,scoring = 'accuracy',cv = crossvalidation,n_jobs = 1))\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f49c1b",
   "metadata": {},
   "source": [
    "you can see that the results are similar.This is just additional information that the gradient boosting model does outperform the baseline decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b9ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
